{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Following - Live Demo\n",
    "\n",
    "In this notebook we'll show how you can follow an object with JetBot!  We'll use a pre-trained neural network\n",
    "that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects.  These include\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\n",
    "which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\n",
    "\n",
    "This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\n",
    "\n",
    "Anyways, let's get started.  First, we'll want to import the ``ObjectDetector`` class which takes our pre-trained SSD engine.\n",
    "\n",
    "\n",
    "# 오브젝트 따라가기 - 라이브 데모\n",
    "\n",
    "이 노트북에서는 JetBot로 개체를 따라가는 방법을 보여줍니다! 우리는 미리 훈련 된 신경망을 사용할 것입니다\n",
    "[COCO dataset] (http://cocodataset.org)에서 90 개의 서로 다른 공통 객체를 감지하도록 훈련했습니다. 이들은 포함합니다\n",
    "\n",
    "* 사람 (인덱스 0)\n",
    "* 컵 (색인 47)\n",
    "\n",
    "그리고 다른 많은 것들 ([이 파일] (https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt)에서 클래스 인덱스의 전체 목록을 확인할 수 있습니다). 모델은 [TensorFlow 객체 감지 API] (https://github.com/tensorflow/models/tree/master/research/object_detection)에서 제공됩니다.\n",
    "사용자 지정 작업을위한 물체 탐지기 교육용 유틸리티도 제공합니다! 모델 학습이 완료되면 Jetson Nano에서 NVIDIA TensorRT를 사용하여 모델을 최적화합니다.\n",
    "\n",
    "이로 인해 Jetson Nano에서 네트워크를 매우 빠르게 실행하고 실시간으로 실행할 수 있습니다! 하지만이 노트북에서는 모든 교육 및 최적화 단계를 수행하지는 않습니다.\n",
    "\n",
    "어쨌든 시작합시다. 먼저 사전 훈련 된 SSD 엔진을 사용하는``ObjectDetector ''클래스를 가져와야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute detections on single camera image\n",
    "\n",
    "## 싱글 카메라 이미지에서 Compute 감지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ssd_mobilenet_v2_coco.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-78c468b59bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjetbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mObjectDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ssd_mobilenet_v2_coco.engine'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetbot-0.3.0-py3.6.egg/jetbot/object_detection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine_path, preprocess_fn)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mload_plugins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],\n\u001b[0;32m---> 28\u001b[0;31m                                   output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jetbot-0.3.0-py3.6.egg/jetbot/tensorrt_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, engine_path, input_names, output_names, final_shapes)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRuntime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_cuda_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_execution_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ssd_mobilenet_v2_coco.engine'"
     ]
    }
   ],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the ``ObjectDetector`` class uses the TensorRT Python API to execute the engine that we provide.  It also takes care of preprocessing the input to the neural network, as\n",
    "well as parsing the detected objects.  Right now it will only work for engines created using the ``jetbot.ssd_tensorrt`` package. That package has the utilities for converting\n",
    "the model from the TensorFlow object detection API to an optimized TensorRT engine.\n",
    "\n",
    "Next, let's initialize our camera.  Our detector takes 300x300 pixel input, so we'll set this when creating the camera.\n",
    "\n",
    "> Internally, the Camera class uses GStreamer to take advantage of Jetson Nano's Image Signal Processor (ISP).  This is super fast and offloads\n",
    "> a lot of the resizing computation from the CPU. \n",
    "\n",
    "내부적으로``ObjectDetector ''클래스는 TensorRT Python API를 사용하여 우리가 제공하는 엔진을 실행합니다. 또한 신경망에 대한 입력을 사전 처리합니다.\n",
    "탐지 된 개체를 파싱 할 수 있습니다. 현재는``jetbot.ssd_tensorrt ''패키지를 사용하여 생성 된 엔진에서만 작동합니다. 이 패키지에는 변환을위한 유틸리티가 있습니다\n",
    "TensorFlow 객체 감지 API에서 최적화 된 TensorRT 엔진으로의 모델.\n",
    "\n",
    "다음으로 카메라를 초기화하겠습니다. 검출기는 300x300 픽셀 입력을 사용하므로 카메라를 만들 때이를 설정합니다.\n",
    "\n",
    "> 내부적으로 Camera 클래스는 GStreamer를 사용하여 Jetson Nano의 이미지 신호 프로세서 (ISP)를 활용합니다. 이것은 매우 빠르고 오프로드입니다.\n",
    "> CPU에서 많은 크기 조정 계산."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's execute our network using some camera input.  By default the ``ObjectDetector`` class expects ``bgr8`` format that the camera produces.  However,\n",
    "you could override the default pre-processing function if your input is in a different format.\n",
    "\n",
    "이제 카메라 입력을 사용하여 네트워크를 실행 해 봅시다. 기본적으로``ObjectDetector ''클래스는 카메라가 생성하는``bgr8 ''형식을 필요로합니다. 하나,\n",
    "입력이 다른 형식 인 경우 기본 전처리 기능을 무시할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any COCO objects in the camera's field of view, they should now be stored in the ``detections`` variable.\n",
    "\n",
    "\n",
    "카메라의 시야에 COCO 객체가있는 경우 이제``detections ''변수에 저장해야합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display detections in text area\n",
    "\n",
    "We'll use the code below to print out the detected objects.\n",
    "\n",
    "\n",
    "### 텍스트 영역에 감지 표시\n",
    "\n",
    "아래 코드를 사용하여 감지 된 개체를 인쇄합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "\n",
    "detections_widget.value = str(detections)\n",
    "\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the label, confidence, and bounding box of each object detected in each image.  There's only one image (our camera) in this example. \n",
    "\n",
    "\n",
    "To print just the first object detected in the first image, we could call the following\n",
    "\n",
    "> This may throw an error if no objects are detected\n",
    "\n",
    "\n",
    "\n",
    "각 이미지에서 감지 된 각 개체의 레이블, 신뢰도 및 경계 상자가 표시되어야합니다. 이 예에는 하나의 이미지 (카메라) 만 있습니다.\n",
    "\n",
    "\n",
    "첫 번째 이미지에서 감지 된 첫 번째 개체 만 인쇄하려면 다음을 호출하면됩니다.\n",
    "\n",
    "> 물체가 감지되지 않으면 오류가 발생할 수 있습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified class.  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision, this is the 'target' object\n",
    "3.  Steer robot towards target object, otherwise wander\n",
    "4.  If we're blocked by an obstacle, turn left\n",
    "\n",
    "We'll also create some widgets that we'll use to control the target object label, the robot speed, and\n",
    "a \"turn gain\", that will control how fast the robot turns based off the distance between the target object\n",
    "and the center of the robot's field of view. \n",
    "\n",
    "\n",
    "First, let's load our collision detection model.  The pre-trained model is stored in this directory as a convenience, but if you followed\n",
    "the collision avoidance example you may want to use that model if it's better tuned for your robot's environment.\n",
    "\n",
    "\n",
    "\n",
    "### 중앙 물체를 따르도록 로봇 제어\n",
    "\n",
    "이제 로봇이 지정된 클래스의 객체를 따르기를 원합니다. 이를 위해 다음을 수행합니다.\n",
    "\n",
    "1. 지정된 클래스와 일치하는 객체를 감지\n",
    "2. 카메라의 시야 중심에 가장 가까운 물체를 선택하십시오. 이것은 '대상'물체입니다.\n",
    "3. 대상을 향해 로봇을 조종하십시오. 그렇지 않으면 방황하십시오.\n",
    "4. 장애물로 막힌 경우 좌회전\n",
    "\n",
    "또한 대상 객체 레이블, 로봇 속도 및\n",
    "대상 물체 사이의 거리에 따라 로봇이 얼마나 빨리 회전하는지 제어하는 ​​\"회전 게인\"\n",
    "로봇의 시야 중심.\n",
    "\n",
    "\n",
    "먼저 충돌 감지 모델을로드하겠습니다. 사전 훈련 된 모델은 편의를 위해이 디렉토리에 저장되지만\n",
    "충돌 회피 예제 로봇 환경에 더 잘 맞는 모델을 사용하고 싶을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "collision_model.load_state_dict(torch.load('../collision_avoidance/best_model.pth'))\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's initialize our robot so we can control the motors.\n",
    "\n",
    "좋습니다. 이제 로봇을 초기화하여 모터를 제어 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display all the control widgets and connect the network execution function to the camera updates.\n",
    "\n",
    "\n",
    "마지막으로 모든 제어 위젯을 표시하고 네트워크 실행 기능을 카메라 업데이트에 연결하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.4, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    # turn left if blocked\n",
    "    if prob_blocked > 0.5:\n",
    "        robot.left(0.3)\n",
    "        image_widget.value = bgr8_to_jpeg(image)\n",
    "        return\n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # otherwise go forward if no target detected\n",
    "    if det is None:\n",
    "        robot.forward(float(speed_widget.value))\n",
    "        \n",
    "    # otherwsie steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance from center\n",
    "        center = detection_center(det)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the block below to connect the execute function to each camera frame update.\n",
    "\n",
    "\n",
    "\n",
    "아래 블록을 호출하여 각 카메라 프레임 업데이트에 실행 기능을 연결하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!  If the robot is not blocked you should see boxes drawn around the detected objects in blue.  The target object (which the robot follows) will be displayed in green.\n",
    "\n",
    "The robot should steer towards the target when it is detected.  If it is blocked by an object it will simply turn left.\n",
    "\n",
    "You can call the code block below to manually disconnect the processing from the camera and stop the robot.\n",
    "\n",
    "\n",
    "대박! 로봇이 막히지 않으면 감지 된 물체 주위에 파란색 상자가 표시됩니다. 로봇이 따르는 대상 물체는 녹색으로 표시됩니다.\n",
    "\n",
    "로봇은 물체가 감지 될 때 목표쪽으로 향해야합니다. 물체에 의해 막히면 왼쪽으로 회전합니다.\n",
    "\n",
    "아래 코드 블록을 호출하여 카메라에서 처리를 수동으로 분리하고 로봇을 중지 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
